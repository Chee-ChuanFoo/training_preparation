{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdaca0e0",
   "metadata": {},
   "source": [
    "# Module 6: Automating Data Pipelines\n",
    "\n",
    "Learn to automate your data analysis workflows and create scheduled reports!\n",
    "\n",
    "## Learning Objectives\n",
    "- Convert notebooks to Python scripts\n",
    "- Create reusable functions\n",
    "- Build ETL (Extract, Transform, Load) pipelines\n",
    "- Schedule automated tasks\n",
    "- Generate automated reports\n",
    "- Best practices for production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8cdb1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "print(\"Libraries loaded!\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709637fb",
   "metadata": {},
   "source": [
    "## 1. From Notebook to Script\n",
    "\n",
    "Jupyter notebooks are great for exploration, but scripts are better for automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00224be3",
   "metadata": {},
   "source": [
    "### Key Differences\n",
    "\n",
    "**Jupyter Notebooks:**\n",
    "- Interactive exploration\n",
    "- Mix code, visualizations, and narrative\n",
    "- Cell-by-cell execution\n",
    "- Great for analysis and presentation\n",
    "\n",
    "**Python Scripts:**\n",
    "- Automated execution\n",
    "- Run start to finish\n",
    "- Can be scheduled\n",
    "- Better for production pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7bdbc",
   "metadata": {},
   "source": [
    "## 2. Creating Reusable Functions\n",
    "\n",
    "Functions make your code modular and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42799d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Simple function example\n",
    "def calculate_revenue(quantity, price):\n",
    "    \"\"\"Calculate total revenue from quantity and price\"\"\"\n",
    "    return quantity * price\n",
    "\n",
    "# Test the function\n",
    "result = calculate_revenue(100, 29.99)\n",
    "print(f\"Revenue: ${result:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2534dca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function with data cleaning\n",
    "def clean_customer_data(df):\n",
    "    \"\"\"\n",
    "    Clean customer data\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with customer data\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Standardize text fields\n",
    "    df_clean['state'] = df_clean['state'].str.upper()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean['phone'] = df_clean['phone'].fillna('Not provided')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates(subset=['customer_id'])\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653858c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function with analysis\n",
    "def analyze_sales_by_region(df):\n",
    "    \"\"\"\n",
    "    Analyze sales by region\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with sales data\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'total_by_region': df.groupby('region')['total_amount'].sum().to_dict(),\n",
    "        'avg_by_region': df.groupby('region')['total_amount'].mean().to_dict(),\n",
    "        'count_by_region': df.groupby('region').size().to_dict(),\n",
    "        'top_region': df.groupby('region')['total_amount'].sum().idxmax(),\n",
    "        'total_revenue': df['total_amount'].sum()\n",
    "    }\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1691763",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test the analysis function\n",
    "df_sales = pd.read_csv('../datasets/sales_data.csv', parse_dates=['date'])\n",
    "results = analyze_sales_by_region(df_sales)\n",
    "\n",
    "print(\"Sales Analysis Results:\")\n",
    "print(f\"Total Revenue: ${results['total_revenue']:,.2f}\")\n",
    "print(f\"Top Region: {results['top_region']}\")\n",
    "print(\"\\nRevenue by Region:\")\n",
    "for region, revenue in results['total_by_region'].items():\n",
    "    print(f\"  {region}: ${revenue:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a2e25",
   "metadata": {},
   "source": [
    "## 3. Building an ETL Pipeline\n",
    "\n",
    "ETL = Extract, Transform, Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f35679",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_sales_data(filepath):\n",
    "    \"\"\"\n",
    "    Extract: Load sales data from CSV\n",
    "    \"\"\"\n",
    "    print(f\"Extracting data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath, parse_dates=['date'])\n",
    "    print(f\"  Loaded {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "def transform_sales_data(df):\n",
    "    \"\"\"\n",
    "    Transform: Clean and enrich sales data\n",
    "    \"\"\"\n",
    "    print(\"Transforming data...\")\n",
    "    \n",
    "    # Create a copy\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # Add time features\n",
    "    df_transformed['year'] = df_transformed['date'].dt.year\n",
    "    df_transformed['month'] = df_transformed['date'].dt.month\n",
    "    df_transformed['quarter'] = df_transformed['date'].dt.quarter\n",
    "    df_transformed['day_of_week'] = df_transformed['date'].dt.day_name()\n",
    "    \n",
    "    # Categorize sales\n",
    "    def categorize_sale(amount):\n",
    "        if amount >= 1000:\n",
    "            return 'High'\n",
    "        elif amount >= 500:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    \n",
    "    df_transformed['sale_category'] = df_transformed['total_amount'].apply(categorize_sale)\n",
    "    \n",
    "    # Calculate commission (5%)\n",
    "    df_transformed['commission'] = df_transformed['total_amount'] * 0.05\n",
    "    \n",
    "    # Standardize region names\n",
    "    df_transformed['region'] = df_transformed['region'].str.upper()\n",
    "    \n",
    "    print(f\"  Transformation complete: {len(df_transformed)} records\")\n",
    "    return df_transformed\n",
    "\n",
    "def load_sales_data(df, output_path):\n",
    "    \"\"\"\n",
    "    Load: Save processed data\n",
    "    \"\"\"\n",
    "    print(f\"Loading data to {output_path}...\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"  Saved {len(df)} records\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a6c2f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Run the ETL pipeline\n",
    "def run_sales_etl_pipeline(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Complete ETL pipeline for sales data\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"STARTING ETL PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Extract\n",
    "        df_raw = extract_sales_data(input_file)\n",
    "        \n",
    "        # Transform\n",
    "        df_transformed = transform_sales_data(df_raw)\n",
    "        \n",
    "        # Load\n",
    "        load_sales_data(df_transformed, output_file)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ETL PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return df_transformed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in ETL pipeline: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def868a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Execute the pipeline\n",
    "df_processed = run_sales_etl_pipeline(\n",
    "    '../datasets/sales_data.csv',\n",
    "    '../datasets/sales_processed.csv'\n",
    ")\n",
    "\n",
    "if df_processed is not None:\n",
    "    print(\"\\nProcessed data sample:\")\n",
    "    print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccaf8f",
   "metadata": {},
   "source": [
    "## 4. Generating Automated Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597c208",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_sales_report(df, report_date=None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive sales report\n",
    "    \"\"\"\n",
    "    if report_date is None:\n",
    "        report_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"SALES REPORT - Generated on {report_date}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(\"\\nüìä OVERALL METRICS\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total Revenue:        ${df['total_amount'].sum():>15,.2f}\")\n",
    "    print(f\"Average Transaction:  ${df['total_amount'].mean():>15,.2f}\")\n",
    "    print(f\"Total Transactions:   {len(df):>18,}\")\n",
    "    print(f\"Unique Customers:     {df['customer_id'].nunique():>18,}\")\n",
    "    print(f\"Unique Products:      {df['product_id'].nunique():>18,}\")\n",
    "    \n",
    "    # Regional breakdown\n",
    "    print(\"\\nüåç REGIONAL BREAKDOWN\")\n",
    "    print(\"-\" * 60)\n",
    "    regional_sales = df.groupby('region')['total_amount'].agg(['sum', 'mean', 'count'])\n",
    "    regional_sales.columns = ['Total_Revenue', 'Avg_Transaction', 'Num_Transactions']\n",
    "    regional_sales = regional_sales.sort_values('Total_Revenue', ascending=False)\n",
    "    \n",
    "    for region, row in regional_sales.iterrows():\n",
    "        print(f\"\\n{region}:\")\n",
    "        print(f\"  Total Revenue:     ${row['Total_Revenue']:,.2f}\")\n",
    "        print(f\"  Avg Transaction:   ${row['Avg_Transaction']:,.2f}\")\n",
    "        print(f\"  Transactions:      {int(row['Num_Transactions']):,}\")\n",
    "    \n",
    "    # Top performers\n",
    "    print(\"\\nüèÜ TOP PERFORMERS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Top products\n",
    "    top_products = df.groupby('product_id')['total_amount'].sum().nlargest(5)\n",
    "    print(\"\\nTop 5 Products by Revenue:\")\n",
    "    for i, (product, revenue) in enumerate(top_products.items(), 1):\n",
    "        print(f\"  {i}. {product}: ${revenue:,.2f}\")\n",
    "    \n",
    "    # Top sales reps\n",
    "    top_reps = df.groupby('sales_rep')['total_amount'].sum().nlargest(5)\n",
    "    print(\"\\nTop 5 Sales Representatives:\")\n",
    "    for i, (rep, revenue) in enumerate(top_reps.items(), 1):\n",
    "        print(f\"  {i}. {rep}: ${revenue:,.2f}\")\n",
    "    \n",
    "    # Payment method analysis\n",
    "    print(\"\\nüí≥ PAYMENT METHOD ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    payment_analysis = df.groupby('payment_method').agg({\n",
    "        'total_amount': ['sum', 'count']\n",
    "    })\n",
    "    payment_analysis.columns = ['Total_Revenue', 'Num_Transactions']\n",
    "    payment_analysis = payment_analysis.sort_values('Total_Revenue', ascending=False)\n",
    "    \n",
    "    for method, row in payment_analysis.iterrows():\n",
    "        pct = (row['Total_Revenue'] / df['total_amount'].sum() * 100)\n",
    "        print(f\"{method}: ${row['Total_Revenue']:,.2f} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Time analysis\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        print(\"\\nüìÖ TIME ANALYSIS\")\n",
    "        print(\"-\" * 60)\n",
    "        date_range = f\"{df['date'].min().date()} to {df['date'].max().date()}\"\n",
    "        print(f\"Date Range: {date_range}\")\n",
    "        \n",
    "        if 'month' in df.columns:\n",
    "            monthly = df.groupby('month')['total_amount'].sum()\n",
    "            print(f\"\\nBest Month: Month {monthly.idxmax()} (${monthly.max():,.2f})\")\n",
    "            print(f\"Worst Month: Month {monthly.idxmin()} (${monthly.min():,.2f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"END OF REPORT\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ad880",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Generate report\n",
    "generate_sales_report(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a7c767",
   "metadata": {},
   "source": [
    "## 5. Creating Visualizations for Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe72d69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_sales_dashboard(df, output_file='sales_dashboard.png'):\n",
    "    \"\"\"\n",
    "    Create a visual dashboard for the sales report\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Plot 1: Revenue by Region (Bar)\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    region_sales = df.groupby('region')['total_amount'].sum().sort_values(ascending=False)\n",
    "    ax1.bar(region_sales.index, region_sales.values, color='#2E86AB')\n",
    "    ax1.set_title('Total Revenue by Region', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Revenue ($)')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Key Metrics (Text)\n",
    "    ax2 = fig.add_subplot(gs[0, 2])\n",
    "    ax2.axis('off')\n",
    "    metrics_text = f\"\"\"\n",
    "KEY METRICS\n",
    "\n",
    "Total Revenue:\n",
    "${df['total_amount'].sum():,.0f}\n",
    "\n",
    "Avg Transaction:\n",
    "${df['total_amount'].mean():,.0f}\n",
    "\n",
    "Total Transactions:\n",
    "{len(df):,}\n",
    "\n",
    "Unique Customers:\n",
    "{df['customer_id'].nunique():,}\n",
    "\"\"\"\n",
    "    ax2.text(0.1, 0.5, metrics_text, fontsize=12, verticalalignment='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='#F0F0F0', alpha=0.8))\n",
    "    \n",
    "    # Plot 3: Payment Methods (Pie)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    payment_counts = df['payment_method'].value_counts()\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "    ax3.pie(payment_counts.values, labels=payment_counts.index, autopct='%1.0f%%',\n",
    "            colors=colors, textprops={'fontsize': 9})\n",
    "    ax3.set_title('Payment Methods', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Sales Trend (Line)\n",
    "    ax4 = fig.add_subplot(gs[1, 1:])\n",
    "    daily_sales = df.groupby('date')['total_amount'].sum().sort_index()\n",
    "    ax4.plot(daily_sales.index, daily_sales.values, linewidth=2, color='#2ECC71')\n",
    "    ax4.set_title('Daily Sales Trend', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Sales ($)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Top Products (Horizontal Bar)\n",
    "    ax5 = fig.add_subplot(gs[2, :2])\n",
    "    top_products = df.groupby('product_id')['total_amount'].sum().nlargest(10)\n",
    "    ax5.barh(range(len(top_products)), top_products.values, color='coral')\n",
    "    ax5.set_yticks(range(len(top_products)))\n",
    "    ax5.set_yticklabels(top_products.index)\n",
    "    ax5.set_xlabel('Revenue ($)')\n",
    "    ax5.set_title('Top 10 Products by Revenue', fontsize=12, fontweight='bold')\n",
    "    ax5.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Sales Distribution (Histogram)\n",
    "    ax6 = fig.add_subplot(gs[2, 2])\n",
    "    ax6.hist(df['total_amount'], bins=30, color='#9B59B6', edgecolor='black', alpha=0.7)\n",
    "    ax6.axvline(df['total_amount'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax6.set_xlabel('Amount ($)')\n",
    "    ax6.set_ylabel('Frequency')\n",
    "    ax6.set_title('Transaction Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'SALES DASHBOARD - {datetime.now().strftime(\"%Y-%m-%d\")}',\n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Dashboard saved to: {output_file}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28350d9d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create dashboard\n",
    "create_sales_dashboard(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718009a",
   "metadata": {},
   "source": [
    "## 6. Complete Automated Report Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abdd790",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def automated_report_pipeline(data_file, report_name='Daily_Sales_Report'):\n",
    "    \"\"\"\n",
    "    Complete automated reporting pipeline\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"AUTOMATED REPORT PIPELINE - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and process data\n",
    "        print(\"\\n[1/4] Loading and processing data...\")\n",
    "        df = pd.read_csv(data_file, parse_dates=['date'])\n",
    "        df = transform_sales_data(df)\n",
    "        print(f\"     ‚úì Processed {len(df)} records\")\n",
    "        \n",
    "        # Step 2: Generate text report\n",
    "        print(\"\\n[2/4] Generating text report...\")\n",
    "        report_file = f'{report_name}_{timestamp}.txt'\n",
    "        \n",
    "        # Redirect print output to file\n",
    "        import sys\n",
    "        original_stdout = sys.stdout\n",
    "        with open(report_file, 'w') as f:\n",
    "            sys.stdout = f\n",
    "            generate_sales_report(df)\n",
    "        sys.stdout = original_stdout\n",
    "        print(f\"     ‚úì Report saved to: {report_file}\")\n",
    "        \n",
    "        # Step 3: Generate visualizations\n",
    "        print(\"\\n[3/4] Creating dashboard...\")\n",
    "        dashboard_file = f'{report_name}_Dashboard_{timestamp}.png'\n",
    "        create_sales_dashboard(df, dashboard_file)\n",
    "        print(f\"     ‚úì Dashboard saved to: {dashboard_file}\")\n",
    "        \n",
    "        # Step 4: Export processed data\n",
    "        print(\"\\n[4/4] Exporting processed data...\")\n",
    "        data_export = f'{report_name}_Data_{timestamp}.csv'\n",
    "        df.to_csv(data_export, index=False)\n",
    "        print(f\"     ‚úì Data exported to: {data_export}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"REPORT PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nGenerated files:\")\n",
    "        print(f\"  1. {report_file}\")\n",
    "        print(f\"  2. {dashboard_file}\")\n",
    "        print(f\"  3. {data_export}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the automated report\n",
    "# automated_report_pipeline('../datasets/sales_data.csv')\n",
    "# Commented out to avoid creating multiple files during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a2d68",
   "metadata": {},
   "source": [
    "## 7. Scheduling Automation (Without Airflow)\n",
    "\n",
    "For beginners, we'll use simple scheduling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619c86b",
   "metadata": {},
   "source": [
    "### Windows Task Scheduler\n",
    "\n",
    "```\n",
    "1. Create a batch file (run_report.bat):\n",
    "   ===================================\n",
    "   @echo off\n",
    "   cd C:\\path\\to\\your\\project\n",
    "   python report_script.py\n",
    "   pause\n",
    "   ===================================\n",
    "\n",
    "2. Open Task Scheduler (search in Windows)\n",
    "3. Click \"Create Basic Task\"\n",
    "4. Name: \"Daily Sales Report\"\n",
    "5. Trigger: Daily at 8:00 AM\n",
    "6. Action: Start a program\n",
    "7. Program: C:\\path\\to\\run_report.bat\n",
    "8. Finish\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c170c73a",
   "metadata": {},
   "source": [
    "### Mac/Linux Cron Job\n",
    "\n",
    "```\n",
    "1. Open terminal and type: crontab -e\n",
    "\n",
    "2. Add this line (runs daily at 8 AM):\n",
    "   0 8 * * * cd /path/to/project && python report_script.py\n",
    "\n",
    "3. Save and exit\n",
    "\n",
    "Cron syntax: minute hour day month day_of_week\n",
    "Examples:\n",
    "- 0 8 * * *        # Daily at 8 AM\n",
    "- 0 8 * * 1        # Every Monday at 8 AM\n",
    "- 0 8 1 * *        # First day of month at 8 AM\n",
    "- */30 * * * *     # Every 30 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75401fe6",
   "metadata": {},
   "source": [
    "## 8. Error Handling and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2ecf0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging(log_file='pipeline.log'):\n",
    "    \"\"\"\n",
    "    Set up logging for the pipeline\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getlogger(__name__)\n",
    "\n",
    "def robust_etl_pipeline(input_file, output_file):\n",
    "    \"\"\"\n",
    "    ETL pipeline with error handling and logging\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    \n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"Starting ETL Pipeline\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Extract\n",
    "        logger.info(f\"Extracting data from {input_file}\")\n",
    "        df = pd.read_csv(input_file, parse_dates=['date'])\n",
    "        logger.info(f\"Successfully loaded {len(df)} records\")\n",
    "        \n",
    "        # Transform\n",
    "        logger.info(\"Transforming data...\")\n",
    "        df = transform_sales_data(df)\n",
    "        logger.info(\"Transformation complete\")\n",
    "        \n",
    "        # Validate\n",
    "        logger.info(\"Validating data...\")\n",
    "        assert len(df) > 0, \"No data to process\"\n",
    "        assert 'total_amount' in df.columns, \"Missing total_amount column\"\n",
    "        logger.info(\"Validation passed\")\n",
    "        \n",
    "        # Load\n",
    "        logger.info(f\"Saving data to {output_file}\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logger.info(\"Data saved successfully\")\n",
    "        \n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(\"ETL Pipeline Completed Successfully\")\n",
    "        logger.info(\"=\"*50)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(\"Input file is empty\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293f606",
   "metadata": {},
   "source": [
    "## 9. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848816d7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a configuration dictionary\n",
    "config = {\n",
    "    'data_sources': {\n",
    "        'sales': '../datasets/sales_data.csv',\n",
    "        'customers': '../datasets/customer_data.csv',\n",
    "        'products': '../datasets/product_catalog.csv'\n",
    "    },\n",
    "    'output_directory': './reports/',\n",
    "    'schedule': {\n",
    "        'frequency': 'daily',\n",
    "        'time': '08:00'\n",
    "    },\n",
    "    'email': {\n",
    "        'enabled': False,\n",
    "        'recipients': ['manager@company.com'],\n",
    "        'subject': 'Daily Sales Report'\n",
    "    },\n",
    "    'thresholds': {\n",
    "        'low_sales_alert': 1000,\n",
    "        'high_value_transaction': 5000\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_config(config_file='config.json'):\n",
    "    \"\"\"\n",
    "    Load configuration from JSON file\n",
    "    \"\"\"\n",
    "    import json\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return config\n",
    "\n",
    "def save_config(config, config_file='config.json'):\n",
    "    \"\"\"\n",
    "    Save configuration to JSON file\n",
    "    \"\"\"\n",
    "    import json\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    print(f\"Configuration saved to {config_file}\")\n",
    "\n",
    "# Example usage\n",
    "# save_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d724f",
   "metadata": {},
   "source": [
    "## 10. Best Practices for Production Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ab40b",
   "metadata": {},
   "source": [
    "### 1. Use Functions and Classes\n",
    "- Break code into small, reusable functions\n",
    "- Each function should do one thing well\n",
    "- Use meaningful names\n",
    "\n",
    "### 2. Add Documentation\n",
    "```python\n",
    "def process_data(df, threshold=100):\n",
    "    \"\"\"\n",
    "    Process sales data above threshold\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): Input sales data\n",
    "    - threshold (float): Minimum sales amount\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Filtered and processed data\n",
    "    \"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 3. Handle Errors\n",
    "- Use try/except blocks\n",
    "- Log errors for debugging\n",
    "- Fail gracefully\n",
    "\n",
    "### 4. Test Your Code\n",
    "- Test with sample data\n",
    "- Handle edge cases\n",
    "- Validate outputs\n",
    "\n",
    "### 5. Use Version Control\n",
    "- Use Git to track changes\n",
    "- Commit regularly with meaningful messages\n",
    "- Create branches for new features\n",
    "\n",
    "### 6. Keep Secrets Safe\n",
    "- Don't hard-code passwords\n",
    "- Use environment variables\n",
    "- Use configuration files (not in Git)\n",
    "\n",
    "### 7. Monitor and Log\n",
    "- Log important events\n",
    "- Monitor pipeline execution\n",
    "- Set up alerts for failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14230bab",
   "metadata": {},
   "source": [
    "## Complete Example: Production-Ready Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1fb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for automated sales reporting\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    INPUT_FILE = '../datasets/sales_data.csv'\n",
    "    OUTPUT_DIR = './reports/'\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SALES REPORT AUTOMATION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"[1/4] Loading data...\")\n",
    "        df = pd.read_csv(INPUT_FILE, parse_dates=['date'])\n",
    "        print(f\"      ‚úì Loaded {len(df)} records\")\n",
    "        \n",
    "        # Process data\n",
    "        print(\"\\n[2/4] Processing data...\")\n",
    "        df = transform_sales_data(df)\n",
    "        print(f\"      ‚úì Processed successfully\")\n",
    "        \n",
    "        # Generate report\n",
    "        print(\"\\n[3/4] Generating report...\")\n",
    "        generate_sales_report(df)\n",
    "        print(f\"      ‚úì Report generated\")\n",
    "        \n",
    "        # Create visualization\n",
    "        print(\"\\n[4/4] Creating visualizations...\")\n",
    "        dashboard_file = f'{OUTPUT_DIR}dashboard_{timestamp}.png'\n",
    "        create_sales_dashboard(df, dashboard_file)\n",
    "        print(f\"      ‚úì Dashboard saved\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"AUTOMATION COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    # main()  # Uncomment to run\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ec979",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "‚úì **Functions**: Create reusable code blocks  \n",
    "‚úì **ETL Pipelines**: Extract, Transform, Load workflows  \n",
    "‚úì **Automated Reports**: Generate reports programmatically  \n",
    "‚úì **Visualizations**: Create dashboards for reports  \n",
    "‚úì **Scheduling**: Use Task Scheduler or cron for automation  \n",
    "‚úì **Error Handling**: Make code robust with try/except  \n",
    "‚úì **Logging**: Track pipeline execution  \n",
    "‚úì **Best Practices**: Production-ready code guidelines\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice**: Convert your analyses into functions\n",
    "2. **Experiment**: Create your own automated reports\n",
    "3. **Schedule**: Set up a simple automated task\n",
    "4. **Learn More**: Explore advanced topics:\n",
    "   - Plotly for interactive visualizations\n",
    "   - FastAPI for web APIs\n",
    "   - Apache Airflow for complex pipelines\n",
    "   - Docker for deployment\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Pandas Documentation**: https://pandas.pydata.org/docs/\n",
    "- **Matplotlib Gallery**: https://matplotlib.org/stable/gallery/\n",
    "- **Python Automation**: \"Automate the Boring Stuff with Python\"\n",
    "- **Data Pipelines**: \"Data Pipelines Pocket Reference\" by James Densmore\n",
    "\n",
    "### Congratulations! üéâ\n",
    "\n",
    "You've completed the Python Data Analysis for Beginners course!\n",
    "You now have the skills to:\n",
    "- Load and analyze data with Pandas\n",
    "- Create professional visualizations\n",
    "- Clean and transform messy data\n",
    "- Build automated data pipelines\n",
    "\n",
    "Keep practicing and happy analyzing! üìäüêç"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
